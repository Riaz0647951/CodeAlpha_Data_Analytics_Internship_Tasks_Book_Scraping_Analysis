{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOROgzL2Gh2QlcEcXrjeRWK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CodeAlpha Data Analytics Internship TASK 1: Web Scraping**"
      ],
      "metadata": {
        "id": "AyNevKvHdVrh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNJO6ImKc7bS",
        "outputId": "24af5fc8-ebef-4e23-8534-2c5079ce2fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting web scraping...\n",
            "Scraping Page 1 of 50: http://books.toscrape.com/index.html\n",
            "Scraping Page 2 of 50: http://books.toscrape.com/catalogue/page-2.html\n",
            "Scraping Page 3 of 50: http://books.toscrape.com/catalogue/page-3.html\n",
            "Scraping Page 4 of 50: http://books.toscrape.com/catalogue/page-4.html\n",
            "Scraping Page 5 of 50: http://books.toscrape.com/catalogue/page-5.html\n",
            "Scraping Page 6 of 50: http://books.toscrape.com/catalogue/page-6.html\n",
            "Scraping Page 7 of 50: http://books.toscrape.com/catalogue/page-7.html\n",
            "Scraping Page 8 of 50: http://books.toscrape.com/catalogue/page-8.html\n",
            "Scraping Page 9 of 50: http://books.toscrape.com/catalogue/page-9.html\n",
            "Scraping Page 10 of 50: http://books.toscrape.com/catalogue/page-10.html\n",
            "Scraping Page 11 of 50: http://books.toscrape.com/catalogue/page-11.html\n",
            "Scraping Page 12 of 50: http://books.toscrape.com/catalogue/page-12.html\n",
            "Scraping Page 13 of 50: http://books.toscrape.com/catalogue/page-13.html\n",
            "Scraping Page 14 of 50: http://books.toscrape.com/catalogue/page-14.html\n",
            "Scraping Page 15 of 50: http://books.toscrape.com/catalogue/page-15.html\n",
            "Scraping Page 16 of 50: http://books.toscrape.com/catalogue/page-16.html\n",
            "Scraping Page 17 of 50: http://books.toscrape.com/catalogue/page-17.html\n",
            "Scraping Page 18 of 50: http://books.toscrape.com/catalogue/page-18.html\n",
            "Scraping Page 19 of 50: http://books.toscrape.com/catalogue/page-19.html\n",
            "Scraping Page 20 of 50: http://books.toscrape.com/catalogue/page-20.html\n",
            "Scraping Page 21 of 50: http://books.toscrape.com/catalogue/page-21.html\n",
            "Scraping Page 22 of 50: http://books.toscrape.com/catalogue/page-22.html\n",
            "Scraping Page 23 of 50: http://books.toscrape.com/catalogue/page-23.html\n",
            "Scraping Page 24 of 50: http://books.toscrape.com/catalogue/page-24.html\n",
            "Scraping Page 25 of 50: http://books.toscrape.com/catalogue/page-25.html\n",
            "Scraping Page 26 of 50: http://books.toscrape.com/catalogue/page-26.html\n",
            "Scraping Page 27 of 50: http://books.toscrape.com/catalogue/page-27.html\n",
            "Scraping Page 28 of 50: http://books.toscrape.com/catalogue/page-28.html\n",
            "Scraping Page 29 of 50: http://books.toscrape.com/catalogue/page-29.html\n",
            "Scraping Page 30 of 50: http://books.toscrape.com/catalogue/page-30.html\n",
            "Scraping Page 31 of 50: http://books.toscrape.com/catalogue/page-31.html\n",
            "Scraping Page 32 of 50: http://books.toscrape.com/catalogue/page-32.html\n",
            "Scraping Page 33 of 50: http://books.toscrape.com/catalogue/page-33.html\n",
            "Scraping Page 34 of 50: http://books.toscrape.com/catalogue/page-34.html\n",
            "Scraping Page 35 of 50: http://books.toscrape.com/catalogue/page-35.html\n",
            "Scraping Page 36 of 50: http://books.toscrape.com/catalogue/page-36.html\n",
            "Scraping Page 37 of 50: http://books.toscrape.com/catalogue/page-37.html\n",
            "Scraping Page 38 of 50: http://books.toscrape.com/catalogue/page-38.html\n",
            "Scraping Page 39 of 50: http://books.toscrape.com/catalogue/page-39.html\n",
            "Scraping Page 40 of 50: http://books.toscrape.com/catalogue/page-40.html\n",
            "Scraping Page 41 of 50: http://books.toscrape.com/catalogue/page-41.html\n",
            "Scraping Page 42 of 50: http://books.toscrape.com/catalogue/page-42.html\n",
            "Scraping Page 43 of 50: http://books.toscrape.com/catalogue/page-43.html\n",
            "Scraping Page 44 of 50: http://books.toscrape.com/catalogue/page-44.html\n",
            "Scraping Page 45 of 50: http://books.toscrape.com/catalogue/page-45.html\n",
            "Scraping Page 46 of 50: http://books.toscrape.com/catalogue/page-46.html\n",
            "Scraping Page 47 of 50: http://books.toscrape.com/catalogue/page-47.html\n",
            "Scraping Page 48 of 50: http://books.toscrape.com/catalogue/page-48.html\n",
            "Scraping Page 49 of 50: http://books.toscrape.com/catalogue/page-49.html\n",
            "Scraping Page 50 of 50: http://books.toscrape.com/catalogue/page-50.html\n",
            "\n",
            "--- Task 1 Complete ---\n",
            "Successfully scraped 1000 books.\n",
            "Data saved to books_data.csv\n",
            "First 5 rows of your data:\n",
            "                                   Title  Price Availability  Rating_Stars\n",
            "0                   A Light in the Attic  51.77     In stock             3\n",
            "1                     Tipping the Velvet  53.74     In stock             1\n",
            "2                             Soumission  50.10     In stock             1\n",
            "3                          Sharp Objects  47.82     In stock             4\n",
            "4  Sapiens: A Brief History of Humankind  54.23     In stock             5\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re # For regex operations\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = 'http://books.toscrape.com/catalogue/page-{}.html'\n",
        "TOTAL_PAGES = 50 # The website has 50 pages of books\n",
        "scraped_data = []\n",
        "# --- Helper Function: Convert text rating to numeric value ---\n",
        "def convert_rating(rating_text):\n",
        "    \"\"\"Converts the star rating class name (e.g., 'Three') to a number (e.g., 3).\"\"\"\n",
        "    # The rating is stored as a class name like 'star-rating Three'\n",
        "    # We use a dictionary to map the text to a numerical value\n",
        "    rating_map = {\n",
        "        'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5\n",
        "    }\n",
        "    # Find the class that contains the rating text (e.g., 'Three')\n",
        "    for key, value in rating_map.items():\n",
        "        if key in rating_text:\n",
        "            return value\n",
        "    return 0 # Return 0 if no rating is found\n",
        "\n",
        "# --- Main Scraping Logic ---\n",
        "print(\"Starting web scraping...\")\n",
        "for page_num in range(1, TOTAL_PAGES + 1):\n",
        "    # Construct the URL for the current page\n",
        "    # The first page is an exception: http://books.toscrape.com/index.html\n",
        "    # Subsequent pages follow the pattern: http://books.toscrape.com/catalogue/page-N.html\n",
        "    if page_num == 1:\n",
        "        url = 'http://books.toscrape.com/index.html'\n",
        "    else:\n",
        "        url = BASE_URL.format(page_num)\n",
        "\n",
        "    print(f\"Scraping Page {page_num} of {TOTAL_PAGES}: {url}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Send the HTTP request\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "\n",
        "        # 2. Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # 3. Find all book containers on the page\n",
        "        # Each book listing is wrapped in an <article class=\"product_pod\"> tag\n",
        "        books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "        if not books:\n",
        "            print(\"No more books found. Stopping.\")\n",
        "            break\n",
        "\n",
        "        # 4. Extract data for each book\n",
        "        for book in books:\n",
        "            # Title (inside h3 > a tag, stored in the 'title' attribute)\n",
        "            title = book.h3.a['title']\n",
        "\n",
        "            # Price (inside p tag with class 'price_color')\n",
        "            # Use regex to clean the currency symbol (e.g., 'Â£51.77' -> 51.77)\n",
        "            price_element = book.find('p', class_='price_color').text.strip()\n",
        "            price_match = re.search(r'[\\d\\.]+', price_element)\n",
        "            price = float(price_match.group(0)) if price_match else None\n",
        "\n",
        "            # Availability (inside p tag with class 'instock availability')\n",
        "            availability = book.find('p', class_='instock availability').text.strip()\n",
        "            # Clean up the text: 'In stock (22 available)' -> 'In stock'\n",
        "            availability = availability.split('(')[0].strip()\n",
        "\n",
        "            # Rating (inside p tag with a class that starts with 'star-rating')\n",
        "            rating_class = book.find('p', class_=re.compile(r'star-rating'))['class']\n",
        "            rating = convert_rating(rating_class)\n",
        "\n",
        "            # Store the data in the list\n",
        "            scraped_data.append({\n",
        "                'Title': title,\n",
        "                'Price': price,\n",
        "                'Availability': availability,\n",
        "                'Rating_Stars': rating\n",
        "            })\n",
        "\n",
        "        # Be a good web citizen: wait a small amount of time before the next request\n",
        "        time.sleep(1)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error accessing {url}: {e}. Skipping page.\")\n",
        "        continue\n",
        "\n",
        "# 5. Convert the list of dictionaries to a Pandas DataFrame\n",
        "df = pd.DataFrame(scraped_data)\n",
        "\n",
        "# 6. Save the DataFrame to a CSV file\n",
        "output_file = 'books_data.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"\\n--- Task 1 Complete ---\")\n",
        "print(f\"Successfully scraped {len(df)} books.\")\n",
        "print(f\"Data saved to {output_file}\")\n",
        "print(\"First 5 rows of your data:\")\n",
        "print(df.head())"
      ]
    }
  ]
}